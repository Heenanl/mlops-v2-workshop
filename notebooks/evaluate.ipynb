{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675840381150
        }
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.pyfunc\n",
        "from mlflow.tracking import MlflowClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1675840381325
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "TARGET_COL = \"cost\"\n",
        "\n",
        "NUMERIC_COLS = [\n",
        "    \"distance\", \"dropoff_latitude\", \"dropoff_longitude\", \"passengers\", \"pickup_latitude\",\n",
        "    \"pickup_longitude\", \"pickup_weekday\", \"pickup_month\", \"pickup_monthday\", \"pickup_hour\",\n",
        "    \"pickup_minute\", \"pickup_second\", \"dropoff_weekday\", \"dropoff_month\", \"dropoff_monthday\",\n",
        "    \"dropoff_hour\", \"dropoff_minute\", \"dropoff_second\"\n",
        "]\n",
        "\n",
        "CAT_NOM_COLS = [\n",
        "    \"store_forward\", \"vendor\"\n",
        "]\n",
        "\n",
        "CAT_ORD_COLS = [\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1675840381429
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m.update(kwargs)\n\u001b[32m      7\u001b[39m args = MyArgs(\n\u001b[32m      8\u001b[39m                 model_name = \u001b[33m\"\u001b[39m\u001b[33mtaxi-model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m                 model_input = \u001b[33m\"\u001b[39m\u001b[33m/tmp/train\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m                 test_data = \u001b[33m\"\u001b[39m\u001b[33m/tmp/prep/test\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m                 evaluation_output = \u001b[33m\"\u001b[39m\u001b[33m/tmp/evaluate\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m                 )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mos\u001b[49m.makedirs(args.evaluation_output, exist_ok = \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# Define Arguments for this step\n",
        "\n",
        "class MyArgs:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "args = MyArgs(\n",
        "                model_name = \"taxi-model\",\n",
        "                model_input = \"/tmp/train\",\n",
        "                test_data = \"/tmp/prep/test\",\n",
        "                evaluation_output = \"/tmp/evaluate\"\n",
        "                )\n",
        "\n",
        "os.makedirs(args.evaluation_output, exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675840381537
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def main(args):\n",
        "    '''Read trained model and test dataset, evaluate model and save result'''\n",
        "\n",
        "    # Load the test data\n",
        "    test_data = pd.read_parquet(Path(args.test_data)/\"test.parquet\")\n",
        "\n",
        "    # Split the data into inputs and outputs\n",
        "    y_test = test_data[TARGET_COL]\n",
        "    X_test = test_data[NUMERIC_COLS + CAT_NOM_COLS + CAT_ORD_COLS]\n",
        "\n",
        "    # Load the model from input port\n",
        "    model =  mlflow.sklearn.load_model(args.model_input) \n",
        "\n",
        "    # ---------------- Model Evaluation ---------------- #\n",
        "    yhat_test, score = model_evaluation(X_test, y_test, model, args.evaluation_output)\n",
        "\n",
        "    # ----------------- Model Promotion ---------------- #\n",
        "    predictions, deploy_flag = model_promotion(args.model_name, args.evaluation_output, X_test, y_test, yhat_test, score)\n",
        "\n",
        "\n",
        "\n",
        "def model_evaluation(X_test, y_test, model, evaluation_output):\n",
        "\n",
        "    # Get predictions to y_test (y_test)\n",
        "    yhat_test = model.predict(X_test)\n",
        "\n",
        "    # Save the output data with feature columns, predicted cost, and actual cost in csv file\n",
        "    output_data = X_test.copy()\n",
        "    output_data[\"real_label\"] = y_test\n",
        "    output_data[\"predicted_label\"] = yhat_test\n",
        "    output_data.to_csv((Path(evaluation_output) / \"predictions.csv\"))\n",
        "\n",
        "    # Evaluate Model performance with the test set\n",
        "    r2 = r2_score(y_test, yhat_test)\n",
        "    mse = mean_squared_error(y_test, yhat_test)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, yhat_test)\n",
        "\n",
        "    # Print score report to a text file\n",
        "    (Path(evaluation_output) / \"score.txt\").write_text(\n",
        "        f\"Scored with the following model:\\n{format(model)}\"\n",
        "    )\n",
        "    with open((Path(evaluation_output) / \"score.txt\"), \"a\") as outfile:\n",
        "        outfile.write(\"Mean squared error: {mse.2f} \\n\")\n",
        "        outfile.write(\"Root mean squared error: {rmse.2f} \\n\")\n",
        "        outfile.write(\"Mean absolute error: {mae.2f} \\n\")\n",
        "        outfile.write(\"Coefficient of determination: {r2.2f} \\n\")\n",
        "\n",
        "    mlflow.log_metric(\"test r2\", r2)\n",
        "    mlflow.log_metric(\"test mse\", mse)\n",
        "    mlflow.log_metric(\"test rmse\", rmse)\n",
        "    mlflow.log_metric(\"test mae\", mae)\n",
        "\n",
        "    # Visualize results\n",
        "    plt.scatter(y_test, yhat_test,  color='black')\n",
        "    plt.plot(y_test, y_test, color='blue', linewidth=3)\n",
        "    plt.xlabel(\"Real value\")\n",
        "    plt.ylabel(\"Predicted value\")\n",
        "    plt.title(\"Comparing Model Predictions to Real values - Test Data\")\n",
        "    plt.savefig(\"predictions.png\")\n",
        "    mlflow.log_artifact(\"predictions.png\")\n",
        "\n",
        "    return yhat_test, r2\n",
        "\n",
        "def model_promotion(model_name, evaluation_output, X_test, y_test, yhat_test, score):\n",
        "    \n",
        "    scores = {}\n",
        "    predictions = {}\n",
        "\n",
        "    client = MlflowClient()\n",
        "\n",
        "    for model_run in client.search_model_versions(f\"name='{model_name}'\"):\n",
        "        model_version = model_run.version\n",
        "        mdl = mlflow.pyfunc.load_model(\n",
        "            model_uri=f\"models:/{model_name}/{model_version}\")\n",
        "        predictions[f\"{model_name}:{model_version}\"] = mdl.predict(X_test)\n",
        "        scores[f\"{model_name}:{model_version}\"] = r2_score(\n",
        "            y_test, predictions[f\"{model_name}:{model_version}\"])\n",
        "\n",
        "    if scores:\n",
        "        if score >= max(list(scores.values())):\n",
        "            deploy_flag = 1\n",
        "        else:\n",
        "            deploy_flag = 0\n",
        "    else:\n",
        "        deploy_flag = 1\n",
        "    print(f\"Deploy flag: {deploy_flag}\")\n",
        "\n",
        "    with open((Path(evaluation_output) / \"deploy_flag\"), 'w') as outfile:\n",
        "        outfile.write(f\"{int(deploy_flag)}\")\n",
        "\n",
        "    # add current model score and predictions\n",
        "    scores[\"current model\"] = score\n",
        "    predictions[\"currrent model\"] = yhat_test\n",
        "\n",
        "    perf_comparison_plot = pd.DataFrame(\n",
        "        scores, index=[\"r2 score\"]).plot(kind='bar', figsize=(15, 10))\n",
        "    perf_comparison_plot.figure.savefig(\"perf_comparison.png\")\n",
        "    perf_comparison_plot.figure.savefig(Path(evaluation_output) / \"perf_comparison.png\")\n",
        "\n",
        "    mlflow.log_metric(\"deploy flag\", bool(deploy_flag))\n",
        "    mlflow.log_artifact(\"perf_comparison.png\")\n",
        "\n",
        "    return predictions, deploy_flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1675840385832
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model name: taxi-model\n",
            "Model path: /tmp/train\n",
            "Test data path: /tmp/prep/test\n",
            "Evaluation output path: /tmp/evaluate\n",
            "artifact uri: file:///c:/Users/heenarefai/Documents/code/mlops-v2-workshop/notebooks/mlruns/775451619847611530/35b866ba498b4be08f6faf010d4a4831/artifacts\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'main' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=\u001b[33m\"\u001b[39m\u001b[33mevaluate-nyc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     14\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33martifact uri:\u001b[39m\u001b[33m'\u001b[39m, mlflow.get_artifact_uri())  \n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m   \u001b[43mmain\u001b[49m(args)\n\u001b[32m     17\u001b[39m mlflow.end_run()\n",
            "\u001b[31mNameError\u001b[39m: name 'main' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "lines = [\n",
        "    f\"Model name: {args.model_name}\",\n",
        "    f\"Model path: {args.model_input}\",\n",
        "    f\"Test data path: {args.test_data}\",\n",
        "    f\"Evaluation output path: {args.evaluation_output}\",\n",
        "]\n",
        "\n",
        "for line in lines:\n",
        "    print(line)\n",
        "\n",
        "### !!!! TODO !! Replace ith your name `my-experiment-john`\n",
        "mlflow.set_experiment(\"heena-test\")\n",
        "with mlflow.start_run(run_name=\"evaluate-nyc\"):\n",
        "  print('artifact uri:', mlflow.get_artifact_uri())  \n",
        "  main(args)\n",
        "\n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675840387138
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deploy_flag  perf_comparison.png  predictions.csv  score.txt\r\n"
          ]
        }
      ],
      "source": [
        "ls \"/tmp/evaluate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "mlops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
